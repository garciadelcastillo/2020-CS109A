{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Standard Section 2: kNN and Linear Regression\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Hayden Joy, Lauren Baker, and Kaela Nelson<br/>\n",
    "\n",
    "\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, our goal is to get you familiarized with k-Nearest Neighbors (kNN) and Linear Regression. In the course thus far, we have discussed some aspects of dealing with data, including scraping data from the web, organizing it using dictionaries and Pandas dataframes, and visualizing it using Matplotlib plotting functionality. Now we're moving on to data modeling!\n",
    "\n",
    "It is useful to make models to fit and predict data. **Why?** To understand the underlying behavior of your data.\n",
    "\n",
    "By the end of this section, you should feel comfortable: \n",
    "    \n",
    "- Performing exploratory data analysis (EDA) on dataset\n",
    "- Splitting this dataset into a training and test set (and understanding why you need to do this!)\n",
    "- Applying simple models (kNN and Linear Regression) to your data using sklearn and statsmodels packages\n",
    "- Using these models to understand relationships between the response variable and the predictors (also can be called features or descriptors)\n",
    "- Evaluating model performance using metrics such as $R^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrices, Dataframe and Plotting Operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Model packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from statsmodels.api import OLS\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Bikeshare dataset and perform preliminary EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will work with a dataset collected from the Capital Bikeshare program in Washington D.C. This dataset contains over two years of data on the total number of bike rentals per day, as well as 10 attributes describing the day and its weather (see below for description of these variables as recorded in the dataset). The data set is provided in the file 'bikeshare.csv'.\n",
    "\n",
    "The task is to build a regression model to **predict the total number of bike rentals in a given day** (known as the response variable) based on attributes about the day (known as the descriptors). Such a demand forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city.\n",
    "\n",
    "**Description of variables**\n",
    "\n",
    "- season (1 = spring, 2 = summer, 3 = fall, 4 = winter)\n",
    "- month (1 through 12, with 1 denoting Jan)\n",
    "- holiday (1 = the day is a holiday, 0 = otherwise)\n",
    "- day_of_week (0 through 6, with 0 denoting Sunday)\n",
    "- workingday (1 = the day is neither a holiday or weekend, 0 = otherwise)\n",
    "- weather \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp (temperature in Celsius)\n",
    "- atemp (apparent, or relative outdoor, or real feel temperature, in Celsius)\n",
    "- humidity (relative humidity)\n",
    "- windspeed (wind speed)\n",
    "- **count** (response variable i.e. total number of bike rentals on the day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the BikeShare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare = pd.read_csv('../data/bikeshare.csv')\n",
    "print(\"Length of Dataset:\",len(bikeshare))\n",
    "display(bikeshare.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop unnecessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare = bikeshare.drop(columns=['Unnamed: 0'])\n",
    "print(\"Length of Dataset:\",len(bikeshare))\n",
    "display(bikeshare.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the describe feature of Pandas to summarize data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bikeshare.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note, we can also use the groupby function to look at mean stats, aggregated by month in this case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare.groupby('month').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the count of bike rentals by month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(bikeshare.groupby('month').mean()['count'],'-ob')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Total Count of Bikeshare Rentals per Month')\n",
    "plt.xlim([0,13])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a difference between temp and a_temp? Let's plot them both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(bikeshare['temp'], bikeshare['atemp'],'.-b', alpha = 0.5) # toggle alpha to 1\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs. Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do wrong here? Why does the plot look like this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting!** Whenever your plot makes zig-zag changes across the scale, it is because ```matplotlib``` is trying to connect the points *sequentially* from the top (using a line plot) and skipping across the scale when $x_{i+1}$ is lower than $x_{i}$. So let's sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "new = bikeshare.sort_values(['temp'])\n",
    "\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(new['temp'], new['atemp'],'-b',alpha=1)\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It still looks weird, why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ```atemp``` values for each ```temp``` value, which if not sorted will bounce around at the same x-value. Thus, we need to sort both axes simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = bikeshare.sort_values(['temp','atemp'])\n",
    "\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(new['temp'], new['atemp'],'-b')\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting efficiently, we found an anomaly we would have otherwise overlooked. It looks like there is a problem with the data around ```temp greater than 30``` and ```atemp less than 10```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show all rows in the dataframe where the temp is greater than 30 and the atemp is less than 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bikeshare[(bikeshare['temp'] > 30) & (bikeshare['atemp'] < 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly! ```atemp``` and ```temp``` are usually lineary related except at this one datapoint. Now, we get to make a judgement call as to whether we should keep the datapoint. For this example, we will drop this datapoint, but we will come back to the question of how to manage abnormal/missing data after the lecture on Missing Data and Imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikeshare = bikeshare.drop([188])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try what we wrote and we should end up with no rows in the dataframe where the temp is greater than 30 and the atemp is less than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bikeshare[(bikeshare['temp'] > 30) & (bikeshare['atemp'] < 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This EDA enabled us to spot this anomoly and clean the data (dropping a data point that may have influenced the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting up the data into a training set and a test set using the 'train_test_split' function from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what the data looks like, we want to predict the count. We will be randomly splitting up the data into a **training** and a **testing** set. [Scikit learn (sklearn)](https://scikit-learn.org/stable/index.html) has a function that does this for us, called \"train_test_split.\"\n",
    "\n",
    "What is the need for training and testing data sets?\n",
    "The **training** set will be used to train the model, while the **testing** set will be used to quantify how well that model does on data it has never seen before. Evaluating the accuracy of a model on a **testing** set ensures that the model doesn't overfit our training data.\n",
    "\n",
    "**Why random split?** Randomization helps manage uncontrollable confounding variables within the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us first create a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(bikeshare, test_size=0.30, random_state=42) \n",
    "\n",
    "print(\"Length of Training set = \",len(train_data))\n",
    "print(\"Length of Testing set  = \",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random state of 42 is arbitrary (Pavlos favorite number, google for \"the number of universe\") but fixing this value will produce the same randomization of data every time (useful for homework grading, research, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the ratio of the number of points in the training set to the number of points in the testing set to see if we have split the data correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training data length is the', 100*len(train_data)/len(bikeshare),'% of the total dataset length.')\n",
    "print('The testing  data length is the', 100*len(test_data)/len(bikeshare),'% of the total dataset length.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Break Out Room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Practice using train_test_split and visualizing data using matplotlib**\n",
    "\n",
    "**Directions:**\n",
    "1. Load \"sim_data.csv\" file into dataframe\n",
    "    - Inspect data\n",
    "    - Drop any unncessary columns\n",
    "1. Split data into 80% training data and 20% test data (\"80-20 train-test split\")\n",
    "1. Plot training and test data in single plot\n",
    "    - Color training data in blue and test data in red. Also, use different markers for the two sets, e.g. o and *\n",
    "    - Include a legend, title, and axes labels! \n",
    "    \n",
    "*Hint: dont forget to sort!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/breakout_1_sol.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's do some modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General sklearn model fitting code-structure :**\n",
    "\n",
    "```\n",
    "#Split Data into Train and Test Set\n",
    "x_train, y_train = training_data.drop('Response_Variable', axis=1), training_data['Response_Variable']\n",
    "x_test, y_test = test_data.drop('Response_Variable', axis=1), test_data['Response_Variable']\n",
    "\n",
    "#Define Model\n",
    "model = sklearn_model_name(hyper_parameter1 = value1, hyper_parameter2 = value2)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Get Prediction\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Evaluate Model\n",
    "r2_train = model.score(y_train, y_pred_train)\n",
    "r2_test = model.score(y_test, y_pred_test)\n",
    "\n",
    "#Print Results\n",
    "print(\"Score for Model (Training):\", r2_train)\n",
    "print(\"Score for Model (Testing) :\", r2_test)\n",
    "```\n",
    "\n",
    "* Every model has a list of hyperparameters that can be set using sklearn for the specific problem. We find optimal hyperparameters through exploration (one way is cross-validation, which we will discuss soon).\n",
    "\n",
    "* ```model.fit``` calculates the parameters of your model corresponding to the training data and hyperparameters you provided.\n",
    "\n",
    "* ```model.predict(X)``` is the standard method called to make the model predict values for a specific X. Depending on if you feed x_train or x_test, you will get a y_prediction_train or y_prediction_test respectively.\n",
    "\n",
    "* Evaluation of model can vary according to the task at hand i.e. Regression or Classification. For Regression, $R^2$ Score is standard while for Classification, Accuracy (%) is standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Regression\n",
    "\n",
    "![knn](../fig/knn_1.png)\n",
    "![knn](../fig/knn_2.png)\n",
    "![knn](../fig/knn_3.png)\n",
    "\n",
    "\n",
    "## Using sklearn to implement kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the temperature parameter to predict total bike rental count. We can use sklearn to implement kNN, fit the model, and use various metrics to assess our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Set kNN hyperparameter:\n",
    "k = 10\n",
    "\n",
    "# First, we create the classifier object:\n",
    "neighbors = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "# Then, we fit the model using x_train as training data and y_train as target values:\n",
    "neighbors.fit(train_data[['temp']], train_data['count'])\n",
    "\n",
    "# Retrieve our predictions:\n",
    "prediction_knn = neighbors.predict(test_data[['temp']])\n",
    "\n",
    "# This returns the mean accuracy on the given test data and labels, or in other words, \n",
    "# the R squared value -- A constant model that always predicts the expected value of y, \n",
    "# disregarding the input features, would get a R^2 score of 1.\n",
    "r2_train = neighbors.score(train_data[['temp']], train_data['count'])\n",
    "r2_test = neighbors.score(test_data[['temp']], test_data['count'])\n",
    "print(\"Length of Test Data:\", len(test_data['count']))\n",
    "print(\"R^2 Score of kNN on training set:\", r2_train)\n",
    "print(\"R^2 Score of kNN on testing set: \", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubPlots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "\n",
    "# train data\n",
    "axes[0].set_ylim([0,10000])\n",
    "axes[0].plot(train_data['temp'], train_data['count'], 'bo', alpha = 0.5, label = 'Data' )\n",
    "\n",
    "sorted_temp = train_data.sort_values(['temp'])\n",
    "prediction_knn = neighbors.predict(sorted_temp[['temp']])\n",
    "\n",
    "axes[0].plot(sorted_temp['temp'], prediction_knn, 'k-', linewidth = 5, markersize = 10, label = 'Prediction')\n",
    "axes[0].set_xlabel('Temperature', fontsize = 15)\n",
    "axes[0].set_ylabel('# of Rentals', fontsize = 15)\n",
    "axes[0].set_title(\"Train Set\", fontsize = 18)\n",
    "axes[0].legend(loc = 'upper right', fontsize = 12)\n",
    "\n",
    "# test data\n",
    "axes[1].set_ylim([0,10000])\n",
    "axes[1].plot(test_data['temp'], test_data['count'],'r*', alpha = 0.5, label = 'Data' )\n",
    "\n",
    "sorted_temp = test_data.sort_values(['temp'])\n",
    "prediction_knn = neighbors.predict(sorted_temp[['temp']])\n",
    "\n",
    "axes[1].plot(sorted_temp['temp'], prediction_knn, 'g-', linewidth = 5, markersize = 10, label = 'Prediction')\n",
    "axes[1].set_xlabel('Temperature', fontsize = 15)\n",
    "axes[1].set_ylabel('# of Rentals', fontsize = 15)\n",
    "axes[1].set_title(\"Test Set\", fontsize = 18)\n",
    "axes[1].legend(loc = 'upper right', fontsize = 12)\n",
    "\n",
    "fig.suptitle(\"kNN Regression (k={}): Temp vs Rental Count\".format(k), fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Break Out Room 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Practice using sklearn's kNN regression**\n",
    "\n",
    "**Directions:**\n",
    "1. Use same dataset from Break Out Room 1 (\"sim_data.csv\")\n",
    "1. Perform 70-30 train-test split using a random state of 42 \n",
    "1. Create a function that implements kNN regression with your choice of k (explore a few different k's)\n",
    "1. Predict on both training and test data\n",
    "1. For all kNN models generated, plot the following on the same plot:\n",
    "    - Original train data = blue\n",
    "    - Original test data = red\n",
    "    - Predicted train data = black\n",
    "    - Predicted test data = green\n",
    "1. Calculate $R^2$ score \n",
    "\n",
    "*Hints:*\n",
    "- dont forget to sort!\n",
    "- can make plot colors more transparent using \"alpha\" and lines thicker using \"linewidth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/breakout_2_sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "![linear regression](fig/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just went over the kNN prediction method. Now, we will fit the same data using a linear regression model. \n",
    "\n",
    "**What is the main difference between a kNN model and linear regression model?** Linear regression specifies the model (whatever the data is, the model will fit a linear line) whereas kNN learns the model and fits the best curve. \n",
    "\n",
    "Advantages of linear regression models are that they are very fast and yield an exact optimal solution. For a more in-depth discussion on generalized linear models, please see the Advanced Section on this.\n",
    "\n",
    "We will use the same training/testing dataset as before and create linear regression objects. We can do this using sklearn (as we did for kNN) as well as with another package called [statsmodels](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data as X,Y for ease\n",
    "x_train, y_train = train_data['temp'], train_data['count']\n",
    "x_test, y_test = test_data['temp'], test_data['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also split into train test by x and y using train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( bikeshare['temp']), bikeshare['count'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a Linear Regression model using sklearn and take a look at the model's parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_sklearn = LinearRegression().fit(x_train.values.reshape(-1,1), y_train) # x data must be 2D array\n",
    "print('Coefficients:', lr_sklearn.coef_)\n",
    "print('Intercept:', lr_sklearn.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* only one coefficient here since only using one descriptor variable (temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use model to predict on training and testing data and plot prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_preds_train = lr_sklearn.predict(x_train.values.reshape(-1,1))\n",
    "y_preds_test = lr_sklearn.predict(x_test.values.reshape(-1,1))\n",
    "\n",
    "# plot predictions\n",
    "fig, axes = plt.subplots(1,2,figsize=(20,6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].scatter(x_train, y_train, color = 'b', alpha = 0.5, label = 'Data')\n",
    "axes[0].plot(x_train, y_preds_train, 'k', linewidth = 5, label = 'Prediction')\n",
    "axes[0].set_title('Train Set', fontsize = 18)\n",
    "axes[1].scatter(x_test, y_test, color = 'r',marker='*', alpha = 0.5, label = 'Data')\n",
    "axes[1].plot(x_test, y_preds_test, 'g', linewidth = 5, label = 'Prediction')\n",
    "axes[1].set_title('Test Set', fontsize = 18)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_ylim(0,10000)\n",
    "    ax.set_xlabel('Temperature', fontsize = 15)\n",
    "    ax.set_ylabel('# of Rentals', fontsize = 15)\n",
    "    ax.legend(loc = 'upper right', fontsize = 12)\n",
    "\n",
    "fig.suptitle(\"Linear Regression: Temp vs Rental Count\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute performance metrics for both training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "print(\"MSE Train: {:.3f}\".format(metrics.mean_squared_error(y_train, y_preds_train)))\n",
    "print(\"MSE Test: {:.3f}\".format(metrics.mean_squared_error(y_test, y_preds_test)))\n",
    "\n",
    "# R^2 score\n",
    "print(\"R^2 Train: {:.3f}\".format(metrics.r2_score(y_train, y_preds_train)))\n",
    "print(\"R^2 Test: {:.3f}\".format(metrics.r2_score(y_test, y_preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that more accurate models will have higher $R^2$ scores (value of 1 is perfect fitted line) and lower MSEs (meaning lower error). Notice that the $R^2$ for training is higher and MSE is lower than that of the test set, indicating some overfitting to the training set. For more info on these, check out sklearn [metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) documentation. Take a look at the end of the notebook for calculations of MSE and $R^2$ metrics by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a Linear Regression model using statsmodels and print out the coefficients of `temp` and `const`**\n",
    "\n",
    "*Note*: \n",
    "- OLS = ordinary least squares\n",
    "- Must add constants to X data since an intercept is not included by default (unlike sklearn linear regression)\n",
    "- statsmodels OLS first entry is the response variable (Y) followed by X whereas sklearn uses X followed by Y structure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show [1 x]*[beta_0 beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.api import OLS\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# Add constant to x data\n",
    "x_train_ca = sm.add_constant(x_train)\n",
    "x_test_ca = sm.add_constant(x_test)\n",
    "\n",
    "# Create Linear Regression object\n",
    "model = sm.OLS(y_train, x_train_ca) \n",
    "\n",
    "# Fit \n",
    "results = model.fit()\n",
    "print(results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the fitted model (saved as \"results\") to predict on train and test data and plot prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our model\n",
    "fig, axes = plt.subplots(1,2,figsize=(20,6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# train data\n",
    "axes[0].plot(x_train, y_train, 'bo', alpha = 0.5, label = 'Data')\n",
    "sorted_temp = train_data.sort_values(['temp'])\n",
    "prediction_lr = results.predict(sm.add_constant(sorted_temp[['temp']]))\n",
    "axes[0].plot(sorted_temp['temp'], prediction_lr, 'k-', linewidth = 5, label = 'Prediction')\n",
    "axes[0].set_title('Train Set', fontsize = 18)\n",
    "\n",
    "# test data\n",
    "axes[1].plot(x_test, y_test, 'r*', alpha = 0.5, label = 'Data')\n",
    "sorted_temp = test_data.sort_values(['temp'])\n",
    "prediction_lr = results.predict(sm.add_constant(sorted_temp[['temp']]))\n",
    "axes[1].plot(sorted_temp['temp'], prediction_lr, 'g-', linewidth = 5, label = 'Prediction')\n",
    "axes[1].set_title('Test Set', fontsize = 18)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_ylim(0,10000)\n",
    "    ax.set_xlabel('Temperature', fontsize = 15)\n",
    "    ax.set_ylabel('# of Rentals', fontsize = 15)\n",
    "    ax.legend(loc = 'upper right', fontsize = 12)\n",
    "    \n",
    "fig.suptitle('Linear Regression: Temp vs Rental Count (statsmodels)', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out $R^2$ (remember 1 is perfect prediction)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 Score Train (statsmodels linear regression):\", metrics.r2_score(y_train, results.predict(x_train_ca)))\n",
    "print(\"R^2 Score Test (statsmodels linear regression):\", metrics.r2_score(y_test, results.predict(x_test_ca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these $R^2$ scores are indeed the same as what we found above using sklearn's linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out `results.summary()` and see the nice table that statsmodels provides!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Break Out Room 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Compare kNN and linear regression for the same dataset**\n",
    "\n",
    "**Directions:**\n",
    "1. Use same dataset from previous Break Out Rooms, with 70-30 train-test split and random state of 42 \n",
    "1. Create a function that implements linear regression with sklearn or statsmodels\n",
    "1. Predict on both training and test data\n",
    "1. Create 2 subplots with the following plotted:\n",
    "    - Subplot 1: Train set\n",
    "        - Plot training data in blue\n",
    "        - Plot linear regression prediction in black\n",
    "        - Plot kNN prediction (using k = 10) in magenta ('m')\n",
    "    - Subplot 2: Test set\n",
    "        - Plot testing data in red\n",
    "        - Plot linear regression prediction in green\n",
    "        - Plot kNN prediction (using k = 10) in yellow ('y') \n",
    "1. Calculate $R^2$ scores for both train and test sets for both kNN and linear regression \n",
    "\n",
    "*Hints:*\n",
    "- don't forget sort!\n",
    "- plt.subplots(...) creates subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/breakout_3_sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Science, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. Simply speaking, a Confidence Interval is a range of values we are fairly sure our true value lies in. \n",
    "\n",
    "It is important to remind ourselves here that Confidence Intervals belong to a parameter and not a statistic. Thus, they represent the window in which the true value exists for the entire population when all we have is a sample.\n",
    "\n",
    "![ci](../fig/confidence_intervals.png)\n",
    "\n",
    "**See if you can implement a 95% confidence interval using statsmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Interval using Stats Model Summary\n",
    "thresh = 0.05\n",
    "intervals = results.conf_int(alpha=thresh)\n",
    "\n",
    "# Renaming column names\n",
    "first_col = str(thresh/2*100)+\"%\"\n",
    "second_col = str((1-thresh/2)*100)+\"%\"\n",
    "intervals = intervals.rename(columns={0:first_col,1:second_col})\n",
    "display(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, ```results.conf_int(alpha=thresh)``` returns a dataframe with columns 0 and 1. We explained Confidence Intervals above where because we assume normal symetric distribution of data, the 95% Confidence Interval means there's 2.5% chance of the true value lying below the values in Column 0 and 2.5% chance of the true value lying above Column 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### End of Standard Section\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Train-Test Split using a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Split data into Train and Test Set\n",
    "def split_data(data):\n",
    "    \n",
    "    #Calculate Length of Dataset\n",
    "    length = len(data)\n",
    "    \n",
    "    #Define Split\n",
    "    split = 0.7\n",
    "    \n",
    "    #Set a random Seed For Shuffling\n",
    "    np.random.seed(9001)\n",
    "    \n",
    "    #Generate a Mask with a X:Y Split\n",
    "    mask = np.random.rand(length) < split\n",
    "    \n",
    "    #Separate train and test data\n",
    "    data_train = data[mask]\n",
    "    data_test = data[~mask]\n",
    "    \n",
    "    #Return Separately\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data using defined function\n",
    "train_data_manual, test_data_manual = split_data(bikeshare)\n",
    "print(\"Length of Training set:\",len(train_data_manual))\n",
    "print(\"Length of Testing set:\",len(test_data_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that the ratio between test and train sets is right\n",
    "test_data_manual.shape[0]/(test_data_manual.shape[0]+train_data_manual.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Implementing the kNN Algorithm by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN Algorithm\n",
    "def knn_algorithm(train, test, k):\n",
    "    \n",
    "    #Create any empty list to store our predictions in\n",
    "    predictions = []\n",
    "    \n",
    "    #Separate the response and predictor variables from training and test set:\n",
    "    train_x = train['temp']\n",
    "    train_y = train['count']\n",
    "    test_x  = test['temp']\n",
    "    test_y  = test['count']\n",
    "    \n",
    "    for i, ele in enumerate(test_x):\n",
    "        \n",
    "        #For each test point, store the distance between all training points and test point\n",
    "        distances = pd.DataFrame((train_x.values - ele)**2 , index=train.index)\n",
    "        distances.columns =['dist']\n",
    "        \n",
    "        #display(distances)\n",
    "        #Then, we sum across the columns per row to obtain the Euclidean distance squared\n",
    "        ##distances = vec_distances.sum(axis = 1)\n",
    "        \n",
    "        #Sort the distances to training points (in ascending order) and take first k points\n",
    "        nearest_k = distances.sort_values(by='dist').iloc[:k]\n",
    "        \n",
    "        #For simplicity, we omitted the square rooting of the Euclidean distance because the\n",
    "        #square root function preserves order. \n",
    "        \n",
    "        #Take the mean of the y-values of training set corresponding to the nearest k points\n",
    "        k_mean = train_y[nearest_k.index].mean()\n",
    "        \n",
    "        #Add on the mean to our predicted y-value list\n",
    "        predictions.append(k_mean)\n",
    "    \n",
    "    #Create a dataframe with the x-values from test and predicted y-values  \n",
    "    predict = test.copy()  \n",
    "    predict['predicted_count'] = pd.Series(predictions, index=test.index)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the algorithm on our dataset with $k = 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the kNN function \n",
    "\n",
    "k = 5\n",
    "predicted_knn = knn_algorithm(train_data, test_data, k)\n",
    "predicted_knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R^2$ coefficient. Let's create a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test predictions in comparison to true value of test set\n",
    "def evaluate(predicted, true):\n",
    "    \n",
    "    #Find the squared error:\n",
    "    squared_error = (predicted['predicted_count'] - true['count'])**2\n",
    "    \n",
    "    #Finding the mean squared error:\n",
    "    error_var = squared_error.sum()\n",
    "    sample_var = ((true['count'] - true['count'].mean())**2).sum()\n",
    "    r = (1 - (error_var / sample_var))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's apply this function to our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of Test Data:\",len(test_data))\n",
    "print(\"R^2 Score of kNN test:\", evaluate(predicted_knn, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_knn_train = knn_algorithm(test_data, train_data, k)\n",
    "print(\"R^2 Score of kNN train:\", evaluate(predicted_knn_train, train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Computing different performance metrics by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute metrics that can be used to assess fit.\n",
    "\n",
    "**Note: sklearn.metrics is class of functions that consists of all the metrics we care about to evaluate our models. While it is not hard to implement them yourself, it is helpful to go through http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, x_train_ca)\n",
    "results = model.fit()\n",
    "\n",
    "#Predict on train and test\n",
    "y_pred_train = results.predict(x_train_ca)\n",
    "y_pred_test = results.predict(x_test_ca)\n",
    "\n",
    "#Calc squared error\n",
    "squared_error_train = (y_pred_train - y_train)**2\n",
    "squared_error_test = (y_pred_test - y_test)**2\n",
    "\n",
    "#Calc mean squared error\n",
    "error_var_train = squared_error_train.mean()\n",
    "error_var_test = squared_error_test.mean()\n",
    "\n",
    "#Calc variance\n",
    "sample_var_train = ((y_train - y_train.mean())**2).mean()\n",
    "sample_var_test = ((y_test - y_test.mean())**2).mean()\n",
    "\n",
    "#Calc R^2\n",
    "r_sq_train = 1 - error_var_train/sample_var_train\n",
    "r_sq_test = 1 - error_var_test/sample_var_test\n",
    "\n",
    "print('MSE train:', error_var_train, 'R^2 train:', r_sq_train)\n",
    "print('MSE test:', error_var_test, 'R^2 test:', r_sq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
